{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw7_pksgXLLB"
      },
      "source": [
        "# Word Prediction using Markov Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iICP0tELXLLD"
      },
      "source": [
        "This notebook makes use of Markov model for word prediction. Specifically 2nd order Markov model is deployed here for next word prediction. As an example of the Markov chain, an attempt is made to generate a new song lyrics from a bunch of Eminem song lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewTF5EMTXLLE"
      },
      "outputs": [],
      "source": [
        "# Preamble\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKaP7G8rXLLE"
      },
      "outputs": [],
      "source": [
        "# Path of the text file containing the training data\n",
        "training_data_file = 'F:/eminem_songs_lyrics.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWSaVIMWXLLF"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CVJLUTIXLLF"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h8UGuiTXLLF"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(sentence):\n",
        "    return sentence.translate(str.maketrans('','', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2XqYVLkXLLF"
      },
      "outputs": [],
      "source": [
        "def add2dict(dictionary, key, value):\n",
        "    if key not in dictionary:\n",
        "        dictionary[key] = []\n",
        "    dictionary[key].append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc0BvAOMXLLF"
      },
      "outputs": [],
      "source": [
        "def list2probabilitydict(given_list):\n",
        "    probability_dict = {}\n",
        "    given_list_length = len(given_list)\n",
        "    for item in given_list:\n",
        "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
        "    for key, value in probability_dict.items():\n",
        "        probability_dict[key] = value / given_list_length\n",
        "    return probability_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CL82DrVXLLF"
      },
      "outputs": [],
      "source": [
        "initial_word = {}\n",
        "second_word = {}\n",
        "transitions = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfOA2PjWXLLF"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFPAUslwXLLF"
      },
      "outputs": [],
      "source": [
        "# Trains a Markov model based on the data in training_data_file\n",
        "def train_markov_model():\n",
        "    for line in open(training_data_file,encoding='utf-8'):\n",
        "        tokens = remove_punctuation(line.rstrip().lower()).split()\n",
        "        tokens_length = len(tokens)\n",
        "        for i in range(tokens_length):\n",
        "            token = tokens[i]\n",
        "            if i == 0:\n",
        "                initial_word[token] = initial_word.get(token, 0) + 1\n",
        "            else:\n",
        "                prev_token = tokens[i - 1]\n",
        "                if i == tokens_length - 1:\n",
        "                    add2dict(transitions, (prev_token, token), 'END')\n",
        "                if i == 1:\n",
        "                    add2dict(second_word, prev_token, token)\n",
        "                else:\n",
        "                    prev_prev_token = tokens[i - 2]\n",
        "                    add2dict(transitions, (prev_prev_token, prev_token), token)\n",
        "\n",
        "    # Normalize the distributions\n",
        "    initial_word_total = sum(initial_word.values())\n",
        "    for key, value in initial_word.items():\n",
        "        initial_word[key] = value / initial_word_total\n",
        "\n",
        "    for prev_word, next_word_list in second_word.items():\n",
        "        second_word[prev_word] = list2probabilitydict(next_word_list)\n",
        "\n",
        "    for word_pair, next_word_list in transitions.items():\n",
        "        transitions[word_pair] = list2probabilitydict(next_word_list)\n",
        "\n",
        "    print('Training successful.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVbqsLWUXLLG",
        "outputId": "5e85d286-6206-4bca-afba-f43cef8e33c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training successful.\n"
          ]
        }
      ],
      "source": [
        "train_markov_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OGhO62sXLLG",
        "outputId": "a660a45d-d403-4f80-f51c-f56817f7edf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "second_word {'ive': {'been': 0.8333333333333334, 'got': 0.16666666666666666}, 'been': {'a': 1.0}, 'all': {'my': 0.875, 'the': 0.125}, 'well': {'little': 0.6666666666666666, 'to': 0.16666666666666666, 'thats': 0.16666666666666666}, 'if': {'all': 0.6666666666666666, 'that': 0.16666666666666666, 'i': 0.16666666666666666}, 'truth': {'and': 1.0}, 'so': {'let': 0.2857142857142857, 'shes': 0.07142857142857142, 'i': 0.14285714285714285, 'the': 0.07142857142857142, 'here': 0.07142857142857142, 'gay': 0.07142857142857142, 'youll': 0.07142857142857142, 'ray': 0.07142857142857142, 'while': 0.07142857142857142, 'this': 0.07142857142857142}, 'hes': {'comin': 0.16666666666666666, 'nervous': 0.16666666666666666, 'choking': 0.16666666666666666, 'so': 0.3333333333333333, 'grown': 0.16666666666666666}, 'sweat': {'jackets': 1.0}, 'on': {'his': 0.25, 'the': 0.5, 'im': 0.25}, 'and': {'she': 0.041666666666666664, 'guess': 0.041666666666666664, 'you': 0.08333333333333333, 'i': 0.25, 'now': 0.041666666666666664, 'deceit': 0.041666666666666664, 'its': 0.08333333333333333, 'these': 0.041666666666666664, 'at': 0.041666666666666664, 'burst': 0.041666666666666664, 'a': 0.041666666666666664, 'understand': 0.041666666666666664, 'take': 0.041666666666666664, 'make': 0.041666666666666664, 'just': 0.041666666666666664, 'not': 0.041666666666666664, 'when': 0.041666666666666664}, 'its': {'a': 0.2857142857142857, 'hard': 0.14285714285714285, 'actually': 0.14285714285714285, 'me': 0.14285714285714285, 'not': 0.14285714285714285, 'curtains': 0.14285714285714285}, 'shes': {'kept': 1.0}, 'actually': {'just': 1.0}, 'cause': {'she': 0.06666666666666667, 'he': 0.06666666666666667, 'now': 0.06666666666666667, 'man': 0.06666666666666667, 'im': 0.13333333333333333, 'i': 0.26666666666666666, 'fab': 0.06666666666666667, 'sometimes': 0.06666666666666667, 'when': 0.06666666666666667, 'you': 0.06666666666666667, 'in': 0.06666666666666667}, 'but': {'shes': 0.04, 'i': 0.28, 'he': 0.04, 'hold': 0.04, 'im': 0.08, 'for': 0.04, 'as': 0.04, 'its': 0.08, 'sometimes': 0.04, 'look': 0.04, 'in': 0.08, 'if': 0.08, 'fuck': 0.04, 'you': 0.04, 'iâ€™ll': 0.04}, 'knife': {'in': 1.0}, 'says': {'maybe': 1.0}, 'hi': {'suzanne': 1.0}, 'after': {'the': 1.0}, 'the': {'truth': 0.25, 'whole': 0.08333333333333333, 'clocks': 0.08333333333333333, 'souls': 0.08333333333333333, 'way': 0.08333333333333333, 'criminal': 0.08333333333333333, 'presss': 0.08333333333333333, 'fact': 0.08333333333333333, 'track': 0.08333333333333333, 'roof': 0.08333333333333333}, 'onenight': {'stand': 1.0}, 'it': {'was': 0.25, 'dont': 0.25, 'only': 0.25, 'goes': 0.25}, 'he': {'found': 0.125, 'opens': 0.125, 'wont': 0.125, 'knows': 0.125, 'better': 0.125, 'blows': 0.125, 'goes': 0.125, 'nose': 0.125}, 'now': {'how': 0.25, 'who': 0.5, 'i': 0.25}, 'dont': {'want': 0.2, 'take': 0.2, 'have': 0.2, 'mistake': 0.2, 'be': 0.2}, 'i': {'just': 0.05, 'cant': 0.05, 'dont': 0.15, 'suppose': 0.05, 'was': 0.1, 'cannot': 0.05, 'got': 0.1, 'attempt': 0.05, 'get': 0.05, 'know': 0.05, 'make': 0.05, 'bully': 0.05, 'can': 0.05, 'rap': 0.05, 'will': 0.1}, 'irreversible': {'i': 1.0}, 'took': {'advantage': 1.0}, 'why': {'do': 0.5, 'be': 0.5}, 'get': {'on': 0.5, 'outta': 0.5}, 'detergent': {'and': 1.0}, 'we': {'cant': 0.5, 'have': 0.5}, 'to': {'use': 0.25, 'seize': 0.25, 'the': 0.25, 'meet': 0.25}, 'my': {'names': 0.25, 'penll': 0.25, 'honestys': 0.25, 'thoughts': 0.25}, 'river': {'river': 0.5, 'well': 0.5}, 'call': {'me': 1.0}, 'always': {'the': 1.0}, 'fuck': {'can': 1.0}, 'speeds': {'at': 1.0}, 'thats': {'why': 0.6666666666666666, 'all': 0.3333333333333333}, 'this': {'love': 0.125, 'whole': 0.125, 'opportunity': 0.25, 'world': 0.125, 'may': 0.125, 'flippity': 0.125, 'is': 0.125}, 'what': {'else': 0.25, 'i': 0.5, 'is': 0.25}, 'bet': {'i': 1.0}, 'didnt': {'really': 1.0}, 'whats': {'one': 1.0}, 'look': {'if': 0.5, 'i': 0.5}, 'would': {'you': 1.0}, 'his': {'palms': 0.5, 'hoes': 0.5}, 'theres': {'vomit': 1.0}, 'snap': {'back': 1.0}, 'oh': {'there': 0.5, 'hes': 0.5}, 'easy': {'no': 1.0}, 'when': {'he': 0.3333333333333333, 'they': 0.3333333333333333, 'im': 0.3333333333333333}, 'back': {'to': 1.0}, 'you': {'better': 0.11764705882352941, 'own': 0.11764705882352941, 'only': 0.11764705882352941, 'can': 0.058823529411764705, 'are': 0.058823529411764705, 'dont': 0.058823529411764705, 'fags': 0.058823529411764705, 'get': 0.11764705882352941, 'rodent': 0.058823529411764705, 'write': 0.058823529411764705, 'make': 0.058823529411764705, 'hear': 0.058823529411764705, 'hate': 0.058823529411764705}, 'make': {'me': 0.5, 'lemonade': 0.5}, 'a': {'normal': 0.5, 'plaque': 0.5}, 'coast': {'to': 1.0}, 'lonely': {'roads': 1.0}, 'they': {'moved': 0.5, 'said': 0.5}, 'da': {'da': 1.0}, 'no': {'more': 1.0}, 'tear': {'this': 1.0}, 'best': {'believe': 1.0}, 'fact': {'that': 1.0}, 'trying': {'to': 1.0}, 'teeter': {'totter': 1.0}, 'baby': {'mama': 1.0}, 'too': {'much': 1.0}, 'stay': {'in': 1.0}, 'success': {'is': 1.0}, 'mom': {'i': 1.0}, 'feet': {'fail': 1.0}, 'six': {'minutes': 1.0}, 'somethings': {'wrong': 1.0}, 'just': {'a': 0.3333333333333333, 'in': 0.3333333333333333, 'to': 0.3333333333333333}, 'like': {'somethings': 0.5, 'when': 0.5}, 'big': {'trouble': 1.0}, 'im': {'not': 0.1, 'beginning': 0.2, 'an': 0.1, 'the': 0.1, 'out': 0.1, 'a': 0.1, 'devastating': 0.1, 'drunk': 0.1, 'asleep': 0.1}, 'got': {'a': 1.0}, 'made': {'a': 1.0}, 'ever': {'since': 1.0}, 'with': {'monica': 0.25, 'this': 0.25, 'fucking': 0.25, 'rock': 0.25}, 'syllables': {'skillaholic': 1.0}, 'packing': {'a': 1.0}, 'backpack': {'rap': 1.0}, 'ill': {'still': 0.5, 'when': 0.5}, 'over': {'the': 1.0}, 'only': {'realized': 0.5, 'hall': 0.5}, 'how': {'could': 0.25, 'to': 0.25, 'many': 0.25, 'the': 0.25}, 'feel': {'my': 0.5, 'weak': 0.5}, 'rappers': {'are': 1.0}, 'heres': {'a': 0.5, 'what': 0.5}, 'for': {'the': 0.6666666666666666, 'shizzel': 0.3333333333333333}, 'let': {'me': 0.6666666666666666, 'off': 0.3333333333333333}, 'everybody': {'want': 0.5, 'loves': 0.5}, 'immortality': {'like': 1.0}, 'simply': {'rage': 1.0}, 'hit': {'the': 1.0}, 'did': {'nothing': 1.0}, 'mcs': {'get': 1.0}, 'me': {'im': 1.0}, 'lakim': {'shabazz': 1.0}, 'yella': {'eazy': 1.0}, 'inspired': {'enough': 1.0}, 'blow': {'up': 1.0}, 'into': {'the': 1.0}, 'roll': {'hall': 1.0}, 'til': {'i': 1.0}, 'off': {'a': 1.0}, 'tell': {'me': 1.0}, 'little': {'gaylooking': 1.0}, 'youre': {'witnessing': 0.25, 'stuck': 0.25, 'pointless': 0.25, 'coming': 0.25}, 'oy': {'vey': 1.0}, 'hey': {'looking': 0.5, 'fab': 0.5}, 'imma': {'work': 1.0}, 'never': {'asked': 0.5, 'fading': 0.5}, 'basically': {'boy': 1.0}, 'of': {'keeping': 1.0}, 'dale': {'earnhardt': 1.0}, 'kneel': {'before': 1.0}, 'immediately': {'with': 1.0}, 'in': {'the': 1.0}, 'at': {'least': 0.5, 'mayweathers': 0.5}, 'somewhere': {'in': 1.0}, 'enough': {'rhymes': 1.0}, 'maybe': {'try': 1.0}, 'was': {'king': 1.0}, 'appeal': {'with': 1.0}, 'censor': {'you': 1.0}, 'one': {'when': 1.0}, 'put': {'em': 1.0}, 'add': {'an': 1.0}, 'see': {'if': 1.0}, 'that': {'i': 1.0}, 'morphin': {'into': 1.0}, 'while': {'he': 0.5, 'im': 0.5}, 'man': {'oh': 1.0}, 'lyrics': {'coming': 1.0}, 'uh': {'summa': 1.0}, 'innovative': {'and': 1.0}, 'ricocheting': {'off': 1.0}, 'throw': {'on': 1.0}, 'prove': {'that': 1.0}, 'your': {'songs': 1.0}, 'unghh': {'school': 1.0}, 'full': {'of': 1.0}, 'bumping': {'heavy': 1.0}, 'still': {'chunky': 1.0}, 'angels': {'fight': 1.0}, 'theyre': {'asking': 1.0}, 'then': {'you': 1.0}, 'lifes': {'handing': 1.0}, 'think': {'not': 1.0}, 'yo': {'left': 1.0}, 'till': {'i': 0.125, 'the': 0.625, 'my': 0.25}, 'subliminal': {'thoughts': 1.0}, 'adrenaline': {'shots': 1.0}, 'amoxacillin': {'is': 1.0}, 'ima': {'rip': 1.0}, 'music': {'is': 1.0}, 'even': {'though': 1.0}, 'soon': {'as': 1.0}, 'iâ€™ll': {'probably': 1.0}, 'thatâ€™s': {'why': 1.0}, 'until': {'the': 0.5, 'my': 0.5}, 'give': {'out': 1.0}, 'feels': {'like': 1.0}}\n"
          ]
        }
      ],
      "source": [
        "print('second_word',second_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCIdc8PfXLLH"
      },
      "source": [
        "### Generating text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kN51GwlXLLH"
      },
      "outputs": [],
      "source": [
        "def sample_word(dictionary):\n",
        "    p0 = np.random.random()\n",
        "    cumulative = 0\n",
        "    for key, value in dictionary.items():\n",
        "        cumulative += value\n",
        "        if p0 < cumulative:\n",
        "            return key\n",
        "    assert(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx0QiFkYXLLH"
      },
      "outputs": [],
      "source": [
        "number_of_sentences = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eThoqYUaXLLH"
      },
      "outputs": [],
      "source": [
        "# Function to generate sample text\n",
        "def generate():\n",
        "    for i in range(number_of_sentences):\n",
        "        sentence = []\n",
        "        # Initial word\n",
        "        word0 = sample_word(initial_word)\n",
        "        sentence.append(word0)\n",
        "        # Second word\n",
        "        word1 = sample_word(second_word[word0])\n",
        "        sentence.append(word1)\n",
        "        # Subsequent words untill END\n",
        "        while True:\n",
        "            word2 = sample_word(transitions[(word0, word1)])\n",
        "            if word2 == 'END':\n",
        "                break\n",
        "            sentence.append(word2)\n",
        "            word0 = word1\n",
        "            word1 = word2\n",
        "        print(' '.join(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4IAafgRXLLH",
        "outputId": "ba162ebe-ed20-4ca1-8234-5616da9fb192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my thoughts are sporadic i act like iâ€™m an addict\n",
            "so while youâ€™re in it try to get this one chance\n",
            "oh hes too mainstream\n",
            "the roof comes off\n",
            "rappers are having a rough time period\n",
            "the truth and my lies straight\n",
            "i cant keep my lies now are falling like the rain\n",
            "little gaylooking boy\n",
            "my honestys brutal\n",
            "oh hes too mainstream\n"
          ]
        }
      ],
      "source": [
        "generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eEQVCEXXLLH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}